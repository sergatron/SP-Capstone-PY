{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pv_df_orig = pd.read_csv('Capstone/openpv_all.csv', sep=',', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pv_df_orig.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: remove NULL columns\n",
    "def remove_null_col(df, percent=80):\n",
    "  '''Remove all columns with too many null values'''\n",
    "  keeper_list = []\n",
    "  # loser_list = []\n",
    "  col_list = list(df.columns)\n",
    "  # iterate through each column\n",
    "  for col_name in col_list:\n",
    "    # A and B are the conditions for removing columns\n",
    "    nan_count = pd.isnull(df[col_name]).sum()\n",
    "    A = df[col_name].isnull().all()\n",
    "    B = nan_count/len(df.index)*100 > percent\n",
    "    # if column is empty or more than 20% of columns is NaN, then discard, else keep\n",
    "    if (A | B):\n",
    "      # place column into a keeper list\n",
    "      # loser_list.append(col_name)\n",
    "      print('column removed:', col_name)\n",
    "    else:\n",
    "      keeper_list.append(col_name)\n",
    "  #pv_df = df[keeper_list]\n",
    "  return df[keeper_list]\n",
    "pv_df = remove_null_col(pv_df_orig, 80.0)\n",
    "\n",
    "# NOTE: use dropna(axis=1, how='all') to drop entire column\n",
    "pv_df = pv_df[['date_installed', 'city','state','zipcode', 'county', 'size_kw','annual_insolation',\n",
    "               'annual_PV_prod', 'reported_annual_energy_prod',\n",
    "               'cost_per_watt','cost', 'sales_tax_cost', 'rebate', 'incentive_prog_names', 'install_type',\n",
    "               'installer','utility_clean']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT DATA TYPE OF 'ZIPCODES'\n",
    "# convert to numpy array, replace missing values with 0, then convert data type\n",
    "pv_df['zipcode'] = np.nan_to_num(pv_df['zipcode'].values).astype(int)\n",
    "\n",
    "\n",
    "def missing_val_count(df, col_name, show_missing=False):\n",
    "  '''Count missing values and show its index.'''\n",
    "  print('\\nColumn:', col_name, '\\nMissing values:', np.count_nonzero(df.loc[:, col_name].isnull().values))\n",
    "  if show_missing == True:\n",
    "    print(df[df.loc[:, col_name].isnull()])\n",
    "print(missing_val_count(pv_df, 'state'))\n",
    "\n",
    "\n",
    "def drop_n_reset(df):\n",
    "  '''Drop all missing rows, duplicates, and reset the index.'''\n",
    "  df = df.dropna(axis=0, how='all').drop_duplicates()\n",
    "  df = df.reset_index(drop=True)\n",
    "drop_n_reset(pv_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVERT DATA TYPES AND REMOVE STRINGS/SYMBOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER OF STATES\n",
    "print(pv_df.state.nunique()) # number of unique states\n",
    "print(pv_df.state.unique()) # OH, MD and PA contain an empty string, PR and DC are not states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: cost and rebate columns are dtype 'object' although they are supposed to be numeric\n",
    "# test for any digits within character strings\n",
    "col = list(pv_df.columns)\n",
    "for item in col:\n",
    "  if pv_df[item].dtype == np.object:\n",
    "      if pv_df[item].str.isnumeric().any():\n",
    "        print(item, '<- dtype is object but contains digits')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Cost and rebate do not convert due to error. Strings must be replaced before dtype conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pv_df.loc[:, 'cost'] = pv_df.loc[:, 'cost'].values.astype(float)\n",
    "pv_df.loc[:, 'rebate'] = pv_df.loc[:, 'rebate'].values.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNCTION: look for string 'n/a', dollar sign '$', string 'null', and comma ','\n",
    "# strip all leading and trailing whitespaces\n",
    "def remove_symbols(df):\n",
    "  '''For columns which are dtype 'object', strip all leading and trailing whitespaces\n",
    "  remove unwanted strings/symbols such as dollar sign and comma used in currency.\n",
    "  Replace the string 'n/a' and 'null' with a numeric zero'''\n",
    "  cols = list(df.columns)\n",
    "  for item in cols:\n",
    "    if df.loc[:, item].dtype == np.object:\n",
    "      df.loc[:, item] = df.loc[:, item].str.strip('$')\n",
    "      df.loc[:, item] = df.loc[:, item].replace(['n/a', 'null'], 0)\n",
    "    # target specific columns\n",
    "    elif item == 'state':\n",
    "      df.loc[:, 'state'] = df.loc[:, 'state'].str.rstrip()\n",
    "  return df\n",
    "pv_df = remove_symbols(pv_df)\n",
    "\n",
    "pv_df.loc[:, 'rebate'] = pv_df.loc[:, 'rebate'].str.replace(',', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# confirm removal of unwanted characters\n",
    "conditions = [pv_df['cost'].str.contains('n/a').any(), pv_df['rebate'].str.contains('\\$').any(),\n",
    "              pv_df['rebate'].str.contains(',').any(), pv_df['rebate'].str.contains('null').any()]\n",
    "# check for conditions being satisfied\n",
    "for item in conditions:\n",
    "  if item:\n",
    "    print(conditions)\n",
    "    print(\"Test failed. You're not done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert zeros to NaNs since cost cannot be zero\n",
    "pv_df.loc[:, 'cost']= pv_df.loc[:, 'cost'].replace(0, np.nan)\n",
    "\n",
    "# CONVERT TO FLOAT\n",
    "pv_df.loc[:, 'cost'] = pv_df.loc[:, 'cost'].values.astype(float)\n",
    "pv_df.loc[:, 'rebate'] = pv_df.loc[:, 'rebate'].values.astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOWER CASE\n",
    "Convert data types prior to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create function to to test data type and convert to lower case\n",
    "def upper_to_lower(df):\n",
    "  ''' Test each column for data type 'object', then convert to lower case '''\n",
    "\n",
    "  col_name = list(df.columns)\n",
    "  # iterate over columns\n",
    "  for item in col_name:\n",
    "    # if data type is object, then convert column to lower case\n",
    "    if df[item].dtype == np.object:\n",
    "      df.loc[:, item] = df.loc[:, item].str.lower()\n",
    "      print(item, '-converted to lower case')\n",
    "  return df\n",
    "pv_df = upper_to_lower(pv_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ZIPCODES\n",
    "\n",
    "Zipcode column contains to many three and four digit zipcodes, some of which turned out to be invalid. This column will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create missing values in 'state' column then drop rows\n",
    "pv_df[pv_df['zipcode']==0]=np.nan\n",
    "# pv_df['zipcode'] = np.nan_to_num(pv_df['zipcode'].values).astype(int)\n",
    "\n",
    "# count missing values created in the 'state' column\n",
    "missing_val_count(pv_df, 'state', True)\n",
    "\n",
    "# pv_df.info()\n",
    "# drop NA rows, duplicates and reset index\n",
    "pv_df = pv_df.dropna(axis=0, how='all').drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# NOTE: some zipcodes have only 3 or 4 digits with missing zeros. Consider REMOVING\n",
    "pv_df = pv_df.sort_values(by = ['zipcode'])\n",
    "pv_df.loc[1950:2000, 'zipcode']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some zipcodes have only 3 or 4 digits with missing leading zeros as a result they will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select columns to work with further\n",
    "pv_df = pv_df[['date_installed', 'city','state', 'county', 'size_kw','annual_insolation',\n",
    "               'annual_PV_prod', 'reported_annual_energy_prod',\n",
    "               'cost_per_watt','cost','rebate', 'incentive_prog_names', 'install_type',\n",
    "               'installer','utility_clean']]\n",
    "\n",
    "\n",
    "pv_df = pv_df.dropna(axis=0, how='all').drop_duplicates().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# found character '\\r' within 'city' column and removed it\n",
    "pv_df['city'].str.contains('\\r').any()\n",
    "pv_df['city'] = pv_df['city'].str.strip('\\r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# APIs\n",
    "# =============================================================================\n",
    "\n",
    "# CREATE ADDRESS\n",
    "# create city, state combo for use in the APIs\n",
    "df = pv_df[['city', 'state', 'county']]\n",
    "df = pv_df[['state', 'county']].drop_duplicates()\n",
    "len(df)\n",
    "# concat city and state; county and state\n",
    "# create an address list\n",
    "county_state_ls = list(df['county'] + ', ' + df['state'])\n",
    "len(county_state_ls)\n",
    "# create a state list\n",
    "state_ls = list(pv_df['state'].unique())\n",
    "len(state_ls)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SOLAR RADIATION\n",
    "# =============================================================================\n",
    "cities = []\n",
    "states = []\n",
    "solrad_ = []\n",
    "url = 'https://developer.nrel.gov/api/pvwatts/v5.json?api_key={API_KEY}&address={address}&system_capacity={sys_cap}&azimuth={azimuth}&tilt={tilt}&array_type={tracking}&module_type=1&losses=10'\n",
    "for address in county_state_ls:\n",
    "  url_new = url.format(API_KEY='0DkoSjFm3OJ21FRtM05Smfi9bPNoFRcJHpFNgNJw',address=address, sys_cap=sys_cap, azimuth=azimuth, tilt=tilt, tracking=tracking)\n",
    "  # Package the request, send the request and catch the response: r\n",
    "  req = requests.get(url_new)\n",
    "  programs_data = req.json()\n",
    "  try:\n",
    "    city = programs_data['station_info']['city']\n",
    "    state = programs_data['station_info']['state']\n",
    "    solrad = programs_data['outputs']['solrad_annual']\n",
    "    cities.append(city)\n",
    "    states.append(state)\n",
    "    solrad_.append(solrad)\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "sol_df = pd.DataFrame({'cities': cities, 'states':states, 'solar_rad':solrad_})\n",
    "# clean df; drop duplicates, remove underscore, and reset index\n",
    "sol_df = sol_df.drop_duplicates()\n",
    "sol_df['cities'] = sol_df['cities'].str.replace('_', ' ')\n",
    "sol_df = sol_df.reset_index(drop=True)\n",
    "\n",
    "# convert to lower case\n",
    "sol_df = upper_to_lower(sol_df)\n",
    "\n",
    "# write file\n",
    "# sol_df.to_csv('solar_rad_state.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# POPULATION OF CITIES\n",
    "# =============================================================================\n",
    "cities = []\n",
    "states = []\n",
    "population = []\n",
    "for i in range(999):\n",
    "  url = 'https://public.opendatasoft.com/api/records/1.0/search/?dataset=1000-largest-us-cities-by-population-with-geographic-coordinates&rows=1000&sort=-rank&facet=city&facet=state'\n",
    "  req = requests.get(url)\n",
    "  pop_data = req.json()\n",
    "  city = pop_data['records'][i]['fields']['city']\n",
    "  state = pop_data['records'][i]['fields']['state']\n",
    "  pop = pop_data['records'][i]['fields']['population']\n",
    "  cities.append(city)\n",
    "  states.append(state)\n",
    "  population.append(pop)\n",
    "\n",
    "pop_df = pd.DataFrame({'cities': cities, 'states':states, 'population':population})\n",
    "# lower case\n",
    "pop_df = upper_to_lower(pop_df)\n",
    "\n",
    "# wrtie to csv\n",
    "pop_df.to_csv('population_df.csv', index=False)\n",
    "# read csv\n",
    "pop_df = pd.read_csv('population_df.csv', sep=',', low_memory=False)\n",
    "pop_df['states'].unique()\n",
    "pop_df['cities'].nunique()\n",
    "pop_df[pop_df['states']=='district of columbia']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INCENTIVES\n",
    "# =============================================================================\n",
    "\n",
    "incentive_missing = pv_pop[pv_pop['incentive_prog_names'].isnull()]\n",
    "incentives = incentive_missing[['city', 'state', 'incentive_prog_names']].drop_duplicates()\n",
    "address_list = list(incentives['city'] + ', ' + incentives['state'])\n",
    "\n",
    "# extract the name of program and rebate amount\n",
    "state = []\n",
    "incentive = []\n",
    "for item in address_list:\n",
    "  url = 'https://developer.nrel.gov/api/energy_incentives/v2/dsire.json?api_key={API_KEY}&address={address}&category=solar_technologies&technology=solar_photovoltaics'\n",
    "  url_new = url.format(API_KEY='0DkoSjFm3OJ21FRtM05Smfi9bPNoFRcJHpFNgNJw',address=item)\n",
    "  req = requests.get(url_new)\n",
    "  programs_data = req.json()\n",
    "\n",
    "  try:\n",
    "    # iterate over each program\n",
    "    num_prog = len(programs_data['result'])\n",
    "    for i in range(num_prog):\n",
    "      state_name = (programs_data['result'][i]['regions'][0]['name']).lower()\n",
    "      incentive_name = programs_data['result'][i]['program_name']\n",
    "      regions = programs_data['result'][i]['regions'][0]['type']\n",
    "\n",
    "      # extract info: state, and incentive program for state\n",
    "      # if program is for the state (not federal), append info to list\n",
    "      if regions == 'state':\n",
    "        state.append(state_name)\n",
    "        incentive.append(incentive_name)\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "incentive_df = pd.DataFrame({'state': state, 'incentive_program': incentive})\n",
    "incentive_df = incentive_df.drop_duplicates().reset_index(drop=True)\n",
    "# lower case\n",
    "incentive_df['incentive_program'] = incentive_df['incentive_program'].str.lower()\n",
    "# write file\n",
    "incentive_df.to_csv('incentives_state.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataframes obtained from APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_csv(file_name):\n",
    "  df = pd.read_csv(file_name, sep=',', low_memory=False)\n",
    "  return df\n",
    "\n",
    "# open files\n",
    "sol_df = load_csv('solar_rad_state.csv')\n",
    "incent_df = load_csv('incentives_state.csv')\n",
    "states_abbv = load_csv('states_abbreviation.csv')\n",
    "pop_df = load_csv('population_df.csv')\n",
    "\n",
    "# merge pop_df and pv_df, MERGE on STATES\n",
    "pop_df.head()\n",
    "states_abbv.head()\n",
    "pv_df2 = pd.merge(pv_df, states_abbv, left_on='state', right_on='abbreviation', how='left').dropna(axis=0, how='all').drop_duplicates()\n",
    "pv_df2 = pv_df2.reset_index(drop=True)\n",
    "pv_df2.head()\n",
    "pv_df2 = pv_df2[['date_installed', 'city','state', 'full', 'county', 'size_kw','annual_insolation',\n",
    "               'annual_PV_prod', 'reported_annual_energy_prod','cost_per_watt','cost','sales_tax_cost','rebate',\n",
    "               'incentive_prog_names', 'install_type', 'installer','utility_clean']]\n",
    "pv_df2.info()\n",
    "pv_df.info()\n",
    "\n",
    "# merge population\n",
    "pv_df3 = pd.merge(pv_df2, pop_df, left_on=['city','full'], right_on=['cities','states'], how='left')\n",
    "pv_df3 = pv_df3.dropna(axis=0, how='all').drop_duplicates().reset_index(drop=True)\n",
    "pv_df3.info()\n",
    "pv_df3.head()\n",
    "pv_df3 = pv_df3[['date_installed', 'city','state', 'full', 'county', 'population', 'size_kw','annual_insolation',\n",
    "               'annual_PV_prod', 'reported_annual_energy_prod','cost_per_watt','cost', 'sales_tax_cost', 'rebate',\n",
    "               'incentive_prog_names', 'install_type', 'installer','utility_clean']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pv_df = pv_df3\n",
    "# rename columns\n",
    "pv_df3.columns = ['date_installed', 'city', 'state_short', 'state', 'county', 'population',\n",
    "                 'size_kw', 'annual_insolation', 'annual_pv_prod',\n",
    "                 'reported_annual_energy_prod', 'cost_per_watt', 'cost','sales_tax_cost', 'rebate',\n",
    "                 'incentive_prog_names', 'install_type', 'installer', 'utility']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pv_df['county'].str.contains('st. louis').any()\n",
    "pv_df['county'].str.contains('st louis').any()\n",
    "# found both: 'st. louis' and 'st louis' in county column\n",
    "pv_df[pv_df['county']=='st. louis']\n",
    "pv_df[pv_df['state']=='mo']\n",
    "\n",
    "# remove the period from 'st.' to maintain consistency\n",
    "pv_df['city'] = pv_df['city'].str.replace('.', '')\n",
    "pv_df['county'] = pv_df['county'].str.replace('.', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIZE KW\n",
    "# =============================================================================\n",
    "# show summary statistics\n",
    "pv_df.size_kw.describe()\n",
    "missing_val_count(pv_df, 'size_kw', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INCENTIVE_PROG_NAMES\n",
    "# =============================================================================\n",
    "# explore the missing values\n",
    "missing_val_count(pv_df, 'incentive_prog_names', True)\n",
    "pv_df.incentive_prog_names.describe()\n",
    "pv_df.incentive_prog_names.unique()\n",
    "\n",
    "# unique values\n",
    "pv_df['incentive_prog_names'].nunique()\n",
    "\n",
    "# frequency count of programs\n",
    "pv_df.groupby(['incentive_prog_names'])['state'].value_counts().sort_values()\n",
    "\n",
    "\n",
    "# REBATE for each state\n",
    "# median rebate offered by each program\n",
    "pv_df.groupby(['incentive_prog_names'])['rebate'].median().sort_values()\n",
    "\n",
    "pv_df.groupby(['state'])['rebate'].median().sort_values()\n",
    "pv_df.groupby(['state'])['cost'].median().sort_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MISSING VALUES: cost\n",
    "# =============================================================================\n",
    "missing_val_count(pv_df, 'cost', True) # 266,670\n",
    "pv_df['cost'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# # SUMMARY STATS: cost_per_watt\n",
    "# =============================================================================\n",
    "pv_df['cost_per_watt'].describe()\n",
    "missing_val_count(pv_df, 'cost_per_watt', True) # 266,914\n",
    "missing_val_count(pv_df, 'size_kw', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# explore the columns\n",
    "pv_df[['cost_per_watt', 'cost', 'size_kw', 'annual_pv_prod']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MISSING VALUES: cost_per_watt\n",
    "# =============================================================================\n",
    "\n",
    "# FILL COST_PER_WATT\n",
    "# cost per watt = cost / kw*1000\n",
    "# fill in missing COST_PER_WATT based on cost and size\n",
    "pv_df['cost_per_watt'] = pv_df['cost_per_watt'].fillna(pv_df['cost'] / (pv_df['size_kw']*1000))\n",
    "\n",
    "# FILL COST\n",
    "# cost = cost_per_watt * (size*1000)\n",
    "# fill in missing COST values based on SIZE and COST_PER_WATT\n",
    "fill_cost = (pv_df['cost_per_watt']*(pv_df['size_kw']*1000))\n",
    "pv_df['cost'] = pv_df['cost'].fillna(fill_cost)\n",
    "\n",
    "cost = pv_df['cost'] / (pv_df['size_kw']*1000)\n",
    "cost.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# # SUMMARY STATS: annual_PV_prod (estimated production)\n",
    "# =============================================================================\n",
    "\n",
    "# having all values for 'size_kw', fill in missing values in 'annual_pv_prod'\n",
    "pv_df['annual_pv_prod'].describe()\n",
    "missing_val_count(pv_df, 'annual_pv_prod',True) # 226010\n",
    "pv_df[['annual_pv_prod', 'size_kw', 'annual_insolation']]\n",
    "\n",
    "\n",
    "# annual energy production = annual_pv_produced/system_size\n",
    "common_denom = pv_df['annual_pv_prod']/pv_df['size_kw']\n",
    "common_denom.describe()\n",
    "np.count_nonzero(common_denom.isnull().values) # 239608\n",
    "\n",
    "# fill missing values within common_denom using its median\n",
    "common_denom = common_denom.fillna(common_denom.median())\n",
    "\n",
    "# infered annual production = common_denom*size\n",
    "annual_pv_infer = common_denom*pv_df['size_kw']\n",
    "annual_pv_infer.describe()\n",
    "\n",
    "# check the error between actual and calculated energy production\n",
    "error = abs(annual_pv_infer - pv_df['annual_pv_prod'])/pv_df['annual_pv_prod']\n",
    "error.describe()\n",
    "# fill missing values\n",
    "pv_df['annual_pv_prod'] = pv_df['annual_pv_prod'].fillna(annual_pv_infer)\n",
    "pv_df['annual_pv_prod'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY STATS: reported_annual_energy_prod\n",
    "# =============================================================================\n",
    "# count and explore missing values\n",
    "# 833,466 missing values\n",
    "missing_val_count(pv_df, 'reported_annual_energy_prod', True)\n",
    "# this may be too many missing values to fill as it may lead to a lrager error\n",
    "\n",
    "pv_df['reported_annual_energy_prod'].describe()\n",
    "\n",
    "pv_df[['reported_annual_energy_prod', 'annual_pv_prod']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MISSING VALUES: reported_annual_energy_prod\n",
    "# =============================================================================\n",
    "\n",
    "# calculate error between estimated and reported annual PV production\n",
    "error = abs(pv_df['reported_annual_energy_prod'] - pv_df['annual_pv_prod'])/pv_df['annual_pv_prod']\n",
    "error.describe()\n",
    "error.mean() # 0.1083\n",
    "error.median() # 0.0691\n",
    "\n",
    "# fill REPORTED_ANNUAL_ENERGY_PROD missing values based on error from estimated ANNUAL_PV_PROD\n",
    "infer_reported = pv_df['annual_pv_prod']*error.mean()\n",
    "pv_df['reported_annual_energy_prod'] = pv_df['reported_annual_energy_prod'].fillna(infer_reported)\n",
    "pv_df['reported_annual_energy_prod'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY STATS: annual_insolation\n",
    "# use API to fill missing values\n",
    "# =============================================================================\n",
    "missing_val_count(pv_df, 'annual_insolation')\n",
    "pv_df['annual_insolation'].describe()\n",
    "\n",
    "# explore: insolation rate for each state\n",
    "# insolation too high for some states: DO NOT USE\n",
    "state_insol = pv_df.groupby('state')['annual_insolation'].mean()\n",
    "state_insol.sort_values()\n",
    "state_insol.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# install type, consolidate\n",
    "# =============================================================================\n",
    "pv_df['install_type'].unique()\n",
    "\n",
    "pv_df[pv_df['install_type']==\"customer\"]\n",
    "pv_df[pv_df['install_type']==\"unknown\"]\n",
    "pv_df['install_type'].str.contains(\"gov't/np\").value_counts()\n",
    "\n",
    "# consolidate all commercial type\n",
    "comr_ls = ['commerical','commercial - agriculture', 'small business', 'commercial - small business',\n",
    "               'commercial - builders', 'commercial - other', 'commercial']\n",
    "pv_df['install_type'] = pv_df['install_type'].replace(comr_ls, 'commercial')\n",
    "\n",
    "# government\n",
    "pv_df['install_type'] = pv_df['install_type'].replace(\"gov't/np\", 'government')\n",
    "\n",
    "# educational\n",
    "pv_df['install_type'] = pv_df['install_type'].replace(\"education\", 'educational')\n",
    "\n",
    "# agricultural\n",
    "pv_df['install_type'] = pv_df['install_type'].replace(\"agriculture\", 'agricultural')\n",
    "\n",
    "# residential\n",
    "pv_df['install_type'] = pv_df['install_type'].replace('residential/sf', 'residential')\n",
    "\n",
    "# residential\n",
    "pv_df['install_type'] = pv_df['install_type'].replace('not stated', 'unknown')\n",
    "\n",
    "# view the missing values\n",
    "pv_df[pv_df['install_type'].isnull()]\n",
    "\n",
    "# fill missing values with 'unknown'\n",
    "pv_df['install_type'] = pv_df['install_type'].fillna('unknown')\n",
    "\n",
    "pv_df['install_type'] = pv_df['install_type'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fills in missing values for annual solar radiation using the dataframe created through an API for annual \n",
    "# solar radiation as a refrence\n",
    "def reference_fill(df_target, col1, col2, df_ref, col3, col4):\n",
    "  '''Fill in missing values in df_target based on known values from df_ref column'''\n",
    "  # iterate over the known values in reference column\n",
    "  ls = list(df_ref[col3])\n",
    "  for item in df_ref[col3]:\n",
    "    # 1. REFERENCE: find position of unique item in column\n",
    "    # 2. REFERENCE: extract row and contents\n",
    "    # 3. TARGET: find position to fill\n",
    "    # 4. TARGET: fill in value in found position with reference value\n",
    "    \n",
    "    # REFERENCE\n",
    "    try:\n",
    "      row = df_ref[(df_ref[col3]==item)].index[0]\n",
    "      name = df_ref.loc[row, col4] # replacement value\n",
    "      # TARGET\n",
    "      rows = df_target[(df_target.loc[:, col1]==item)].index\n",
    "      df_target.loc[rows, col2] = df_target.loc[rows, col2].fillna(name)\n",
    "    except:\n",
    "      pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INSOLATION\n",
    "sol_df.head()\n",
    "pv_df.head()\n",
    "\n",
    "reference_fill(pv_df, 'state_short', 'annual_insolation', sol_df, 'states', 'solar_rad')\n",
    "missing_val_count(pv_df, 'annual_insolation', True) # 225,394\n",
    "pv_df['annual_insolation'].describe()\n",
    "\n",
    "# missing city for capital, DC\n",
    "# fill with 'washington'\n",
    "missing_val_count(pv_df, 'annual_insolation', True) # missing only in DC\n",
    "pv_df[pv_df['state_short']=='dc']\n",
    "rows = pv_df[(pv_df.loc[:, 'state_short']=='dc')].index\n",
    "pv_df.loc[rows, 'city'] = pv_df.loc[rows, 'city'].fillna('washington')\n",
    "\n",
    "\n",
    "sol_df[sol_df['states']=='dc']\n",
    "sol_df['states'].nunique()\n",
    "sol_df[sol_df['states']=='va']\n",
    "\n",
    "# fill 'insolation' for DC with VA's\n",
    "rows = pv_df[(pv_df.loc[:, 'state_short']=='dc')].index\n",
    "pv_df.loc[rows, 'annual_insolation'] = pv_df.loc[rows, 'annual_insolation'].fillna(4.6741948)\n",
    "missing_val_count(pv_df, 'annual_insolation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pv_df['population'].describe()\n",
    "# median population\n",
    "pv_df['population'].median()\n",
    "\n",
    "# filter DF, filter by population\n",
    "pv_pop = pv_df[pv_df['population']>3.688800e+04]\n",
    "pv_pop = pv_pop.reset_index(drop=True)\n",
    "pv_pop.info()\n",
    "\n",
    "# 'installer' contains mistakes\n",
    "pv_pop['installer'].unique()\n",
    "pv_pop['installer'] = pv_pop['installer'].fillna('unknown')\n",
    "\n",
    "pv_pop['utility'].unique()\n",
    "pv_pop['utility'] = pv_pop['utility'].fillna('unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count missing values\n",
    "missing_val_count(pv_pop, 'incentive_prog_names') # 703\n",
    "missing_val_count(pv_pop, 'annual_insolation') # 6943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill in incentives' missing values\n",
    "# use incent_df as a reference to fill in missing values of incentives for state\n",
    "reference_fill(pv_pop, 'state', 'incentive_prog_names', incent_df, 'state', 'incentive_program')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INSOLATION\n",
    "sol_df.head()\n",
    "pv_df.head()\n",
    "\n",
    "reference_fill(pv_pop, 'state_short', 'annual_insolation', sol_df, 'states', 'solar_rad')\n",
    "missing_val_count(pv_pop, 'annual_insolation', True)\n",
    "pv_df['annual_insolation'].describe()\n",
    "\n",
    "# missing city for washington, DC\n",
    "# fill with 'washington'\n",
    "missing_val_count(pv_df, 'annual_insolation', True) # missing only in DC\n",
    "pv_df[pv_df['state_short']=='dc']\n",
    "rows = pv_df[(pv_df.loc[:, 'state_short']=='dc')].index\n",
    "pv_df.loc[rows, 'city'] = pv_df.loc[rows, 'city'].fillna('washington')\n",
    "\n",
    "# use neighbor's value for annual insolation\n",
    "sol_df[sol_df['states']=='dc']\n",
    "sol_df['states'].nunique()\n",
    "sol_df[sol_df['states']=='va']\n",
    "\n",
    "# fill 'insolation' for DC with VA's\n",
    "rows = pv_df[(pv_df.loc[:, 'state_short']=='dc')].index\n",
    "pv_df.loc[rows, 'annual_insolation'] = pv_df.loc[rows, 'annual_insolation'].fillna(4.6741948)\n",
    "# check again for missing values\n",
    "missing_val_count(pv_df, 'annual_insolation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write file\n",
    "# =============================================================================\n",
    "pv_pop.to_csv('pv_pop_clean.csv', encoding='utf-8', na_rep='NA', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
